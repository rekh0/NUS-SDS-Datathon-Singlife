{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Installing required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install pyarrow\n",
    "#%pip install numpy\n",
    "#%pip install scikit-learn\n",
    "#%pip install imbalanced-learn\n",
    "#%pip install matplotlib\n",
    "#%pip install seaborn\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file\n",
    "df = pd.read_parquet(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Cleaning (Null Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "null_value_proportion = df.isnull().mean()\n",
    "null_value_columns = null_value_proportion[null_value_proportion > threshold].index.tolist()\n",
    "null_value_columns.remove(\"f_purchase_lh\")\n",
    "additional_columns = [\"clntnum\"] # unique identifier of client will not affect their insurance purchase decisions and also high variance\n",
    "columns_to_remove = null_value_columns + additional_columns\n",
    "df = df.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Convert to Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert clients' DOB\n",
    "df['cltdob_fix'] = pd.to_datetime(df['cltdob_fix'], errors='coerce').dt.year\n",
    "\n",
    "\n",
    "# Identify categorical variables\n",
    "categorical_columns = df.select_dtypes(include=[\"object\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Filling in Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.interpolate()\n",
    "\n",
    "# Target column\n",
    "df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0)\n",
    "\n",
    "# Fill in remaining null values with mean\n",
    "target_column = df[\"f_purchase_lh\"]\n",
    "features = df.drop(columns=\"f_purchase_lh\")\n",
    "\n",
    "features = features.fillna(features.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Removing Low-Variance Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_var_columns = []\n",
    "\n",
    "for c in features.columns:\n",
    "    if len(features[c].unique())==1:\n",
    "        low_var_columns += [c]\n",
    "\n",
    "features = features.drop(columns=low_var_columns)\n",
    "\n",
    "cleaned_features = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(features, target_column)\n",
    "\n",
    "importance_scores = decision_tree_model.feature_importances_\n",
    "\n",
    "importance_scores_features = pd.DataFrame({'Feature': features.columns, 'Importance': importance_scores})\n",
    "importance_scores_features = importance_scores_features.sort_values(by = 'Importance', ascending =False)\n",
    "\n",
    "top50_features = importance_scores_features.head(50)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(top50_features['Feature'], top50_features['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance_scores')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Sub-conclusion\n",
    "After cleaning the data and selecting the top 50 features by their importance scores, we conclude that these features will affect customer satisfaction and conversion rates. \n",
    "\n",
    "Hence, Singlife should focus on these variables to enhnace customer experiences and personalise communication.\n",
    "\n",
    "Moving forward, we will be investigating the different machine learning models in predicting customer satisfaction and conversion rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_top50 = features[top50_features[\"Feature\"].tolist()]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_top50, target_column, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "print(\"train accuracy: \", knn_clf.score(X_train, y_train))\n",
    "print(\"val accuracy: \", knn_clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 KNN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "knn_f1scores = cross_val_score(knn_clf, features_top50, target_column, cv=5, scoring='f1_macro')\n",
    "knn_f1scores\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "y_val_pred = knn_clf.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "knn_cm = confusion_matrix(y_val, y_val_pred)\n",
    "knn_disp = ConfusionMatrixDisplay(confusion_matrix=knn_cm,)\n",
    "knn_disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train, y_train)\n",
    "print(\"train accuracy: \", dt_clf.score(X_train, y_train))\n",
    "print(\"val accuracy: \", dt_clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Decision Tree Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_f1scores = cross_val_score(dt_clf, features_top50, target_column, cv=5, scoring='f1_macro')\n",
    "dt_f1scores\n",
    "\n",
    "y_val_pred = dt_clf.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "dt_cm = confusion_matrix(y_val, y_val_pred)\n",
    "dt_disp = ConfusionMatrixDisplay(confusion_matrix=dt_cm,)\n",
    "dt_disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rndforest_clf = RandomForestClassifier()\n",
    "rndforest_clf.fit(X_train, y_train)\n",
    "print(\"train accuracy: \", rndforest_clf.score(X_train, y_train))\n",
    "print(\"val accuracy: \", rndforest_clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Random Forest Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndforest_f1scores = cross_val_score(rndforest_clf, features_top50, target_column, cv=5, scoring='f1_macro')\n",
    "rndforest_f1scores\n",
    "\n",
    "y_val_pred = rndforest_clf.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "rndforest_cm = confusion_matrix(y_val, y_val_pred)\n",
    "rndforest_disp = ConfusionMatrixDisplay(confusion_matrix=rndforest_cm,)\n",
    "rndforest_disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Dense Network Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "dn_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=0)\n",
    "dn_clf.fit(X_train, y_train)\n",
    "print(\"train accuracy: \", dn_clf.score(X_train, y_train))\n",
    "print(\"val accuracy: \", dn_clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Dense Network Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_f1scores = cross_val_score(dn_clf, features_top50, target_column, cv=5, scoring='f1_macro')\n",
    "dn_f1scores\n",
    "\n",
    "y_val_pred = dn_clf.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "dn_cm = confusion_matrix(y_val, y_val_pred)\n",
    "dn_disp = ConfusionMatrixDisplay(confusion_matrix=dn_cm,)\n",
    "dn_disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Conclusion & Final Model\n",
    "\n",
    "We found the Decision tree model to perform the best with the highest F1 score. Hence, it will be our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 train_model Function\n",
    "This function takes in a data set and trains the Decision tree model on that data set. \n",
    "We will call this function in the `testing_hidden_data` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(features_training_set, target_training_set):\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "\n",
    "    dt_model.fit(features_training_set, target_training_set)\n",
    "\n",
    "    return dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 load_model Function\n",
    "This function takes in a data set and make predictions using the Decision tree model from 2.3 that was trained using the data sent to us.\n",
    "\n",
    "This input data set must be cleaned first (i.e. removing NAs, converting to numerical data, filling in NAs) and the top 50 features must be selected. All these follows the same process from 1.1 to 1.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(features_dataset):\n",
    "    return [dt_clf.predict(features_dataset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "\n",
    "    threshold = 0.6\n",
    "    null_value_proportion = hidden_data.isnull().mean()\n",
    "    null_value_columns = null_value_proportion[null_value_proportion > threshold].index.tolist()\n",
    "    additional_columns = [\"clntnum\"] # unique identifier of client will not affect their insurance purchase decisions and also high variance\n",
    "    columns_to_remove = null_value_columns + additional_columns\n",
    "    hidden_data = hidden_data.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Convert clients' DOB\n",
    "    hidden_data['cltdob_fix'] = pd.to_datetime(hidden_data['cltdob_fix'], errors='coerce').dt.year\n",
    "\n",
    "\n",
    "    # Identify categorical variables\n",
    "    categorical_columns = hidden_data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    # Convert using LabelEncoder\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        hidden_data[col] = label_encoder.fit_transform(hidden_data[col])\n",
    "    \n",
    "    hidden_data.interpolate()\n",
    "\n",
    "    hidden_data = hidden_data.fillna(hidden_data.mean())\n",
    "\n",
    "    low_var_columns = []\n",
    "\n",
    "    for c in hidden_data.columns:\n",
    "        if len(hidden_data[c].unique())==1:\n",
    "            low_var_columns += [c]\n",
    "\n",
    "    hidden_data = hidden_data.drop(columns=low_var_columns)\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    decision_tree_model = DecisionTreeClassifier()\n",
    "    decision_tree_model.fit(hidden_data, target_column)\n",
    "\n",
    "    importance_scores = decision_tree_model.feature_importances_\n",
    "\n",
    "    importance_scores_features = pd.DataFrame({'Feature': hidden_data.columns, 'Importance': importance_scores})\n",
    "    importance_scores_features = importance_scores_features.sort_values(by = 'Importance', ascending =False)\n",
    "\n",
    "    top50_features = importance_scores_features.head(50)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    features_top50 = hidden_data[top50_features[\"Feature\"].tolist()]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features_top50, target_column, test_size=0.2, random_state=0)\n",
    "\n",
    "    tm = train_model(X_train, y_train)\n",
    "\n",
    "    result = [tm.predict(X_val)] \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "target_column = test_df[\"f_purchase_lh\"].fillna(0)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
